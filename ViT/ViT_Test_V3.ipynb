{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for VALDO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is based on: https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTForMaskedImageModeling, ViTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level for nibabel.global to WARNING to suppress INFO messages\n",
    "logging.getLogger('nibabel.global').setLevel(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some labels exceed the number of classes. Clamping labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, cases, masks, transform):\n",
    "        self.cases = cases\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "        self.cmb_counts = self.count_cmb_per_image(self.masks)\n",
    "\n",
    "        assert len(self.cases) == len(self.masks), 'Cases and masks must have the same length'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            case = self.cases[idx]\n",
    "            mask = self.masks[idx]\n",
    "\n",
    "            slices = []\n",
    "            masks = []\n",
    "\n",
    "        \n",
    "            s, m = self.transform(mri_image_path=case, segmentation_mask_path=mask)\n",
    "            if s is None or m is None:\n",
    "                raise ValueError(f\"Transform returned None for {case} and {mask}\")\n",
    "            \n",
    "            \n",
    "            slices.append(s)\n",
    "            masks.append(m)\n",
    "            \n",
    "            # print(f'Loaded image {case} and mask {mask}')\n",
    "            return slices, masks, case, self.cmb_counts[idx]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image: {e}')\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            boxes.append([(x-(w/2.5)), (y-(h/2.5)), ((w+x) + (w/3)), ((h+y) + (h/3))])\n",
    "            # boxes.append([x, y, x +     w, y + h])\n",
    "        return boxes\n",
    "\n",
    "    def count_cmb_per_image(self, segmented_images):\n",
    "        cmb_counts = []\n",
    "        for img_path in segmented_images:\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata()\n",
    "            slice_cmb_counts = [self.extract_bounding_boxes(\n",
    "                (data[:, :, i] > 0).astype(np.uint8)) for i in range(data.shape[2])]\n",
    "            total_cmb_count = sum(len(contours)\n",
    "                                  for contours in slice_cmb_counts)\n",
    "            cmb_counts.append(total_cmb_count)\n",
    "        return cmb_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('nibabel').setLevel(logging.WARNING)\n",
    "def load_nifti(file_path):\n",
    "    try:\n",
    "        nifti = nib.load(file_path)\n",
    "        data = nifti.get_fdata()\n",
    "        # print(f\"Loaded NIfTI data shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NIfTI file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "class NiftiToTensorTransform:\n",
    "    def __init__(self, target_shape=(256, 256), in_channels=1):\n",
    "        self.target_shape = target_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(height=target_shape[0], width=target_shape[1], p=1.0, always_apply=True),\n",
    "            ToTensorV2()\n",
    "        ], is_check_shapes=False)  # Disable shape checking if you are sure about your data consistency\n",
    "\n",
    "    def convert_to_binary_mask(self, segmentation_mask):\n",
    "        binary_mask = (segmentation_mask > 0).astype(np.uint8)\n",
    "        return binary_mask\n",
    "\n",
    "    def __call__(self, mri_image_path, segmentation_mask_path):\n",
    "        try:\n",
    "            # Load the images\n",
    "            mri_image = load_nifti(mri_image_path)\n",
    "            segmentation_mask = load_nifti(segmentation_mask_path)\n",
    "            dim_img = nib.load(mri_image_path).header['dim'][0]\n",
    "\n",
    "            if mri_image is None or segmentation_mask is None:\n",
    "                raise ValueError(\"Failed to load NIfTI files or data is None.\")\n",
    "\n",
    "            # Convert multi-label mask to binary mask\n",
    "            binary_mask = self.convert_to_binary_mask(segmentation_mask)\n",
    "\n",
    "            if mri_image.shape[0] != dim_img:\n",
    "                # If the number of channels is not equal to dim, adjust it\n",
    "                mri_image = np.repeat(mri_image, dim_img, axis=0)\n",
    "            \n",
    "            \n",
    "            # Apply transformations to the entire volume\n",
    "            augmented = self.transform(image=mri_image, mask=binary_mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "            # # Ensure the number of channels matches the expected input channels\n",
    "            # if image.shape[0] != self.in_channels:\n",
    "            #     raise ValueError(f\"Expected {self.in_channels} input channels, but got {image.shape[0]} channels. Channels should be {self.in_channels}. MRI Image is {mri_image.shape}\")\n",
    "\n",
    "            \n",
    "            # Debugging prints\n",
    "            # print(f\"Image shape after transform: {image.shape}\")\n",
    "            # print(f\"Mask shape after transform: {mask.shape}\")\n",
    "            # print(f\"Unique values in the transformed mask: {torch.unique(mask)}\")\n",
    "\n",
    "            return image, mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __call__ with {mri_image_path} and {segmentation_mask_path}: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = NiftiToTensorTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_label_relative = '../VALDO_Dataset/Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(\n",
    "    two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(\n",
    "    os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "# print(all_labels[0])\n",
    "# print(all_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cohort1:  11\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cohort1: \", len(cases[\"cohort1\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 3\n",
    "n_classes = 2\n",
    "img_size = (256,256)\n",
    "patch_size = (16,16)\n",
    "n_channels = 1\n",
    "n_heads = 3\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "epochs = 5\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    cmb_counts = []\n",
    "\n",
    "    for item in batch:\n",
    "        if item is not None:  # Skip None items\n",
    "            item_slices, item_targets, item_img_path, item_cmb_counts = item\n",
    "            slices.extend(item_slices)\n",
    "            targets.extend(item_targets)\n",
    "            img_paths.append(item_img_path)\n",
    "            cmb_counts.append(item_cmb_counts)\n",
    "\n",
    "    if slices:\n",
    "        cases = torch.stack(slices, dim=0)\n",
    "        masks = torch.stack(targets, dim=0)\n",
    "        return cases, masks, img_paths, cmb_counts\n",
    "    else:\n",
    "        return None, None, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VALDODataset(\n",
    "    cases=all_ids, masks=all_labels, transform=transform)\n",
    "\n",
    "# dataset = VALDODataset(\n",
    "#     cases=cohort1_ids, masks=cohort1_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.VALDODataset'>\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.cases,\n",
    "    'Segmented Masks': dataset.masks,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})\n",
    "\n",
    "# df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_dataset, test_size=0.2, stratify=df_dataset['Has CMB'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            MRI Scans  \\\n",
      "42  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "20  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "40  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "48  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "23  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "54  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "49  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "46  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "63  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "65  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "1   c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "15  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "30  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "39  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "14  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "\n",
      "                                      Segmented Masks  CMB Count  Has CMB  \n",
      "42  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          4        1  \n",
      "20  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n",
      "40  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n",
      "48  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          1        1  \n",
      "23  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          3        1  \n",
      "54  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          1        1  \n",
      "49  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          3        1  \n",
      "46  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          3        1  \n",
      "63  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          1        1  \n",
      "65  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          2        1  \n",
      "1   c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          1        1  \n",
      "15  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n",
      "30  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n",
      "39  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          3        1  \n",
      "14  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n"
     ]
    }
   ],
   "source": [
    "print(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VALDODataset(train_df['MRI Scans'].tolist(\n",
    "), train_df['Segmented Masks'].tolist(), transform=transform)\n",
    "val_dataset = VALDODataset(val_df['MRI Scans'].tolist(\n",
    "), val_df['Segmented Masks'].tolist(), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_loader, 0): \n",
    "#     print(i)\n",
    "#     inpt = data[0][0][0][34]\n",
    "#     mask = data[1][0][0][34]\n",
    "#     print(\"Input\", len(inpt))\n",
    "#     print(\"Mask\", len(mask))\n",
    "#     print(\"Input\", inpt)\n",
    "#     print(\"Mask\", mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_loader, 0): \n",
    "#     print(i)\n",
    "#     inpt = data[0][0]\n",
    "#     mask = data[1][0][0][4]\n",
    "#     print(\"Input\", len(inpt))\n",
    "#     print(\"Mask\", len(mask))\n",
    "#     print(\"Input\", inpt.shape)\n",
    "#     print(\"Mask\", mask.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], dtype=torch.float64)], [tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)], 'c:\\\\Users\\\\nigel\\\\Documents\\\\Thesis\\\\Thesis_Tests\\\\../VALDO_Dataset/Task2\\\\sub-227\\\\sub-227_space-T2S_desc-masked_T2S.nii.gz', 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: Batch Size <br>\n",
    "**C** : Image channels  <br>\n",
    "**H**: Image Height <br>\n",
    "**W**: Image Width  <br>\n",
    "**P_col**: Patch Columns  <br>\n",
    "**P_row**: Patch Rows  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of Model\n",
    "    self.img_size = img_size # Image Size\n",
    "    self.patch_size = patch_size # Patch Size\n",
    "    self.n_channels = n_channels # Number of Channels\n",
    "\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "  def forward(self, x):\n",
    "    # print(\"inpt is \", x.shape)\n",
    "    x = x.unsqueeze(0)\n",
    "    # print(x.shape)\n",
    "    # Average across the 35-dimension\n",
    "    x = torch.mean(x, dim=2)\n",
    "    x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "\n",
    "    x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "\n",
    "    x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n",
    "\n",
    "    # Creating positional encoding\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Expand to have class token for every image in batch\n",
    "    tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "\n",
    "    # Adding class tokens to the beginning of each embedding\n",
    "    x = torch.cat((tokens_batch,x), dim=1)\n",
    "\n",
    "    # Add positional encoding to embeddings\n",
    "    x = x + self.pe\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    # Sub-Layer 1 Normalization\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "    # Sub-Layer 2 Normalization\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multilayer Perception\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model*r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model*r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Residual Connection After Sub-Layer 1\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "\n",
    "    # Residual Connection After Sub-Layer 2\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    # Sub-Layer 1 Normalization\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "    # Sub-Layer 2 Normalization\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multilayer Perception\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model*r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model*r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Residual Connection After Sub-Layer 1\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "\n",
    "    # Residual Connection After Sub-Layer 2\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes, image_size, patch_size):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size[0]) * (image_size // patch_size[1])  # Corrected num_patches calculation\n",
    "\n",
    "        # Calculate side length of the patches\n",
    "        side_length = int(self.num_patches ** 0.5)\n",
    "        if side_length ** 2 != self.num_patches:\n",
    "            raise ValueError(\"Number of patches is not a perfect square\")\n",
    "\n",
    "        # Define the Conv2d layer\n",
    "        self.conv = nn.Conv2d(embed_dim, num_classes, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Check and move 'self.conv' weights to the same device as 'x'\n",
    "        if x.device != self.conv.weight.device:\n",
    "            self.conv = self.conv.to(x.device)\n",
    "        \n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        expected_patches = self.num_patches  # No additional token in this context\n",
    "        \n",
    "        if num_patches > expected_patches:\n",
    "            x = x[:, :expected_patches, :]\n",
    "        elif num_patches < expected_patches:\n",
    "            padding = torch.zeros(batch_size, expected_patches - num_patches, embed_dim).to(x.device)\n",
    "            x = torch.cat((x, padding), dim=1)\n",
    "\n",
    "        x = x.to(x.device)\n",
    "        x = x.transpose(1, 2)  # Swap dimensions to [batch_size, embed_dim, num_patches]\n",
    "\n",
    "        side_length = int(self.num_patches ** 0.5)\n",
    "        x = x.view(batch_size, embed_dim, side_length, side_length)  # Reshape to [batch_size, embed_dim, sqrt(num_patches), sqrt(num_patches)]\n",
    "        \n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of model\n",
    "    self.n_classes = n_classes # Number of classes\n",
    "    self.img_size = img_size # Image size\n",
    "    self.patch_size = patch_size # Patch size\n",
    "    self.n_channels = n_channels # Number of channels\n",
    "    self.n_heads = n_heads # Number of attention heads\n",
    "\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding( self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n",
    "    self.segmentation_head = SegmentationHead(self.d_model, self.n_classes, self.img_size[0], self.patch_size)\n",
    "\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "\n",
    "    x = self.positional_encoding(x)\n",
    "\n",
    "    x = self.transformer_encoder(x)\n",
    "    \n",
    "    x = self.segmentation_head(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfa6fe2bf724b248c2426fbf04a2847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 loss: 0.000: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Epoch 1/5 loss: 4.532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14185a6940df4d468ceb6145a1038ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 loss: 0.000: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Epoch 2/5 loss: 0.539\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca27570af514b8182d17477ff1ea653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 loss: 0.000: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Epoch 3/5 loss: 2.958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4be0ba4a7fc404b9304eb7ef967d531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 loss: 0.000: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Epoch 4/5 loss: 10.822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f73afd64a0400fba554b7486d80cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 loss: 0.000: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Warning: Some labels exceed the number of classes. Clamping labels.\n",
      "Epoch 5/5 loss: 8.886\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  training_loss = 0.0\n",
    "  for i, data in tqdm(enumerate(train_loader, 0), desc=f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}'):\n",
    "    # print(\"Data: \", data)\n",
    "    inputs = data[0].float()\n",
    "    mask = data[1].float()\n",
    "    inputs, labels = inputs.to(device), mask.to(device)\n",
    "    # labels = labels.squeeze(-1)\n",
    "    labels = torch.argmax(labels, dim=-1)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    outputs = F.interpolate(outputs, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "    # print(\"Outputs: \", outputs)\n",
    "    # print(\"Labels: \", labels)\n",
    "    if labels.max() >= n_classes:\n",
    "      print(\"Warning: Some labels exceed the number of classes. Clamping labels.\")\n",
    "      labels = torch.clamp(labels, 0, n_classes - 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_loss += loss.item()\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = 'experiments'\n",
    "file_num = len([entry for entry in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, entry))])\n",
    "torch.save(transformer.state_dict(), f'experiments/ViT{file_num + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
