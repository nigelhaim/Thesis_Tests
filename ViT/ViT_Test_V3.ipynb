{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for VALDO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is based on: https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTForMaskedImageModeling, ViTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, cases, masks, transform):\n",
    "        self.cases = cases\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "        self.cmb_counts = self.count_cmb_per_image(self.masks)\n",
    "\n",
    "        assert len(self.cases) == len(self.masks), 'Cases and masks must have the same length'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            case = self.cases[idx]\n",
    "            mask = self.masks[idx]\n",
    "\n",
    "            slices = []\n",
    "            masks = []\n",
    "\n",
    "        \n",
    "            s, m = self.transform(mri_image_path=case, segmentation_mask_path=mask)\n",
    "            if s is None or m is None:\n",
    "                raise ValueError(f\"Transform returned None for {case} and {mask}\")\n",
    "            \n",
    "            \n",
    "            slices.append(s)\n",
    "            masks.append(m)\n",
    "            \n",
    "            print(f'Loaded image {case} and mask {mask}')\n",
    "            return slices, masks, case, self.cmb_counts[idx]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image: {e}')\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            boxes.append([(x-(w/2.5)), (y-(h/2.5)), ((w+x) + (w/3)), ((h+y) + (h/3))])\n",
    "            # boxes.append([x, y, x +     w, y + h])\n",
    "        return boxes\n",
    "\n",
    "    def count_cmb_per_image(self, segmented_images):\n",
    "        cmb_counts = []\n",
    "        for img_path in segmented_images:\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata()\n",
    "            slice_cmb_counts = [self.extract_bounding_boxes(\n",
    "                (data[:, :, i] > 0).astype(np.uint8)) for i in range(data.shape[2])]\n",
    "            total_cmb_count = sum(len(contours)\n",
    "                                  for contours in slice_cmb_counts)\n",
    "            cmb_counts.append(total_cmb_count)\n",
    "        return cmb_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    try:\n",
    "        nifti = nib.load(file_path)\n",
    "        data = nifti.get_fdata()\n",
    "        print(f\"Loaded NIfTI data shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NIfTI file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "class NiftiToTensorTransform:\n",
    "    def __init__(self, target_shape=(256, 256), in_channels=1):\n",
    "        self.target_shape = target_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(height=target_shape[0], width=target_shape[1], p=1.0, always_apply=True),\n",
    "            ToTensorV2()\n",
    "        ], is_check_shapes=False)  # Disable shape checking if you are sure about your data consistency\n",
    "\n",
    "    def convert_to_binary_mask(self, segmentation_mask):\n",
    "        binary_mask = (segmentation_mask > 0).astype(np.uint8)\n",
    "        return binary_mask\n",
    "\n",
    "    def __call__(self, mri_image_path, segmentation_mask_path):\n",
    "        try:\n",
    "            # Load the images\n",
    "            mri_image = load_nifti(mri_image_path)\n",
    "            segmentation_mask = load_nifti(segmentation_mask_path)\n",
    "            dim_img = nib.load(mri_image_path).header['dim'][0]\n",
    "\n",
    "            if mri_image is None or segmentation_mask is None:\n",
    "                raise ValueError(\"Failed to load NIfTI files or data is None.\")\n",
    "\n",
    "            # Convert multi-label mask to binary mask\n",
    "            binary_mask = self.convert_to_binary_mask(segmentation_mask)\n",
    "\n",
    "            if mri_image.shape[0] != dim_img:\n",
    "                # If the number of channels is not equal to dim, adjust it\n",
    "                mri_image = np.repeat(mri_image, dim_img, axis=0)\n",
    "            \n",
    "            \n",
    "            # Apply transformations to the entire volume\n",
    "            augmented = self.transform(image=mri_image, mask=binary_mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "            # # Ensure the number of channels matches the expected input channels\n",
    "            # if image.shape[0] != self.in_channels:\n",
    "            #     raise ValueError(f\"Expected {self.in_channels} input channels, but got {image.shape[0]} channels. Channels should be {self.in_channels}. MRI Image is {mri_image.shape}\")\n",
    "\n",
    "            \n",
    "            # Debugging prints\n",
    "            print(f\"Image shape after transform: {image.shape}\")\n",
    "            print(f\"Mask shape after transform: {mask.shape}\")\n",
    "            print(f\"Unique values in the transformed mask: {torch.unique(mask)}\")\n",
    "\n",
    "            return image, mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __call__ with {mri_image_path} and {segmentation_mask_path}: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = NiftiToTensorTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_label_relative = '../VALDO_Dataset/Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(\n",
    "    two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(\n",
    "    os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "# print(all_labels[0])\n",
    "# print(all_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cohort1:  11\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cohort1: \", len(cases[\"cohort1\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 3\n",
    "n_classes = 2\n",
    "img_size = (256,256)\n",
    "patch_size = (16,16)\n",
    "n_channels = 1\n",
    "n_heads = 3\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "epochs = 5\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    cmb_counts = []\n",
    "\n",
    "    for item in batch:\n",
    "        if item is not None:  # Skip None items\n",
    "            item_slices, item_targets, item_img_path, item_cmb_counts = item\n",
    "            slices.extend(item_slices)\n",
    "            targets.extend(item_targets)\n",
    "            img_paths.append(item_img_path)\n",
    "            cmb_counts.append(item_cmb_counts)\n",
    "\n",
    "    if slices:\n",
    "        cases = torch.stack(slices, dim=0)\n",
    "        masks = torch.stack(targets, dim=0)\n",
    "        return cases, masks, img_paths, cmb_counts\n",
    "    else:\n",
    "        return None, None, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "# dataset = VALDODataset(\n",
    "#     cases=all_ids, masks=all_labels, transform=transform)\n",
    "\n",
    "dataset = VALDODataset(\n",
    "    cases=cohort1_ids, masks=cohort1_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.VALDODataset'>\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.cases,\n",
    "    'Segmented Masks': dataset.masks,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})\n",
    "\n",
    "# df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_dataset, test_size=0.2, stratify=df_dataset['Has CMB'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           MRI Scans  \\\n",
      "8  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "9  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "6  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....   \n",
      "\n",
      "                                     Segmented Masks  CMB Count  Has CMB  \n",
      "8  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          0        0  \n",
      "9  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....        100        1  \n",
      "6  c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\....          2        1  \n"
     ]
    }
   ],
   "source": [
    "print(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VALDODataset(train_df['MRI Scans'].tolist(\n",
    "), train_df['Segmented Masks'].tolist(), transform=transform)\n",
    "val_dataset = VALDODataset(val_df['MRI Scans'].tolist(\n",
    "), val_df['Segmented Masks'].tolist(), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_loader, 0): \n",
    "#     print(i)\n",
    "#     inpt = data[0][0][0][34]\n",
    "#     mask = data[1][0][0][34]\n",
    "#     print(\"Input\", len(inpt))\n",
    "#     print(\"Mask\", len(mask))\n",
    "#     print(\"Input\", inpt)\n",
    "#     print(\"Mask\", mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_loader, 0): \n",
    "#     print(i)\n",
    "#     inpt = data[0][0]\n",
    "#     mask = data[1][0][0][4]\n",
    "#     print(\"Input\", len(inpt))\n",
    "#     print(\"Mask\", len(mask))\n",
    "#     print(\"Input\", inpt.shape)\n",
    "#     print(\"Mask\", mask.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NIfTI data shape: (512, 512, 35)\n",
      "Loaded NIfTI data shape: (512, 512, 35)\n",
      "Image shape after transform: torch.Size([35, 256, 256])\n",
      "Mask shape after transform: torch.Size([256, 256, 35])\n",
      "Unique values in the transformed mask: tensor([0, 1], dtype=torch.uint8)\n",
      "Loaded image c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\../VALDO_Dataset/Task2\\sub-108\\sub-108_space-T2S_desc-masked_T2S.nii.gz and mask c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\../VALDO_Dataset/Task2\\sub-108\\sub-108_space-T2S_CMB.nii.gz\n",
      "([tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], dtype=torch.float64)], [tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)], 'c:\\\\Users\\\\nigel\\\\Documents\\\\Thesis\\\\Thesis_Tests\\\\../VALDO_Dataset/Task2\\\\sub-108\\\\sub-108_space-T2S_desc-masked_T2S.nii.gz', 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: Batch Size <br>\n",
    "**C** : Image channels  <br>\n",
    "**H**: Image Height <br>\n",
    "**W**: Image Width  <br>\n",
    "**P_col**: Patch Columns  <br>\n",
    "**P_row**: Patch Rows  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of Model\n",
    "    self.img_size = img_size # Image Size\n",
    "    self.patch_size = patch_size # Patch Size\n",
    "    self.n_channels = n_channels # Number of Channels\n",
    "\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "  def forward(self, x):\n",
    "    print(\"inpt is \", x.shape)\n",
    "    x = x.unsqueeze(0)\n",
    "    print(x.shape)\n",
    "    # Average across the 35-dimension\n",
    "    x = torch.mean(x, dim=2)\n",
    "    x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "\n",
    "    x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "\n",
    "    x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n",
    "\n",
    "    # Creating positional encoding\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Expand to have class token for every image in batch\n",
    "    tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "\n",
    "    # Adding class tokens to the beginning of each embedding\n",
    "    x = torch.cat((tokens_batch,x), dim=1)\n",
    "\n",
    "    # Add positional encoding to embeddings\n",
    "    x = x + self.pe\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    # Sub-Layer 1 Normalization\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "    # Sub-Layer 2 Normalization\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multilayer Perception\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model*r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model*r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Residual Connection After Sub-Layer 1\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "\n",
    "    # Residual Connection After Sub-Layer 2\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    # Sub-Layer 1 Normalization\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "    # Sub-Layer 2 Normalization\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multilayer Perception\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model*r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model*r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Residual Connection After Sub-Layer 1\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "\n",
    "    # Residual Connection After Sub-Layer 2\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of model\n",
    "    self.n_classes = n_classes # Number of classes\n",
    "    self.img_size = img_size # Image size\n",
    "    self.patch_size = patch_size # Patch size\n",
    "    self.n_channels = n_channels # Number of channels\n",
    "    self.n_heads = n_heads # Number of attention heads\n",
    "\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding( self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n",
    "\n",
    "    # Classification MLP\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(self.d_model, self.n_classes),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "\n",
    "    x = self.positional_encoding(x)\n",
    "\n",
    "    x = self.transformer_encoder(x)\n",
    "    \n",
    "    x = self.classifier(x[:,0])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NIfTI data shape: (512, 512, 35)\n",
      "Loaded NIfTI data shape: (512, 512, 35)\n",
      "Image shape after transform: torch.Size([35, 256, 256])\n",
      "Mask shape after transform: torch.Size([256, 256, 35])\n",
      "Unique values in the transformed mask: tensor([0, 1], dtype=torch.uint8)\n",
      "Loaded image c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\../VALDO_Dataset/Task2\\sub-103\\sub-103_space-T2S_desc-masked_T2S.nii.gz and mask c:\\Users\\nigel\\Documents\\Thesis\\Thesis_Tests\\../VALDO_Dataset/Task2\\sub-103\\sub-103_space-T2S_CMB.nii.gz\n",
      "inpt is  torch.Size([1, 35, 256, 256])\n",
      "torch.Size([1, 1, 35, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m transformer(inputs)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  training_loss = 0.0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # print(\"Data: \", data)\n",
    "    inputs = data[0].float()\n",
    "    mask = data[1].float()\n",
    "    inputs, labels = inputs.to(device), mask.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_loss += loss.item()\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
